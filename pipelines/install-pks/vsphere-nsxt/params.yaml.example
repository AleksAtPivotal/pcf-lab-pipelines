# vCenter configuration
vcenter_ca_cert:           # vCenter CA cert at the API endpoint; enter a value if `vcenter_insecure: 0`
vcenter_datacenter: dc01       # vCenter datacenter
vcenter_datastore: datastore1        # vCenter datastore name to deploy Ops Manager in
vcenter_host: vcenter.lab.alekssaul.com           # vCenter host or IP
vcenter_insecure: 1         # vCenter skip TLS cert validation; enter `1` to disable cert verification, `0` to enable verification
vcenter_pwd:                 # vCenter password
vcenter_usr: "Administrator@vsphere.local"            # vCenter username. If user is tied to a domain, then escape the \, example `domain\\user`

nsxt_hostname_or_ipaddress: "nsxmgr.lab.alekssaul.com"
nsxt_admin_username: admin
nsxt_admin_password: 


# vCenter network name to use to deploy Ops Manager in
om_vm_network: PKS-Infra

# vCenter Cluster or Resource Pool to use to deploy Ops Manager.
# Possible formats:
#   cluster:       /<Data Center Name>/host/<Cluster Name>
#   resource pool: /<Data Center Name>/host/<Cluster Name>/Resources/<Resource Pool Name>
om_resource_pool: /dc01/host/cluster01/Resources/pks-az1

# Optional - vCenter folder to put Ops Manager in
# Format: /<Data Center Name>/vm/<folder_name>
om_vm_folder: 

# Optional - vCenter host to deploy Ops Manager in
om_vm_host:

# Name to use for Ops Manager VM
om_vm_name: opsmanpksnsx

# FQDN to access Ops Manager without protocol (will use https), ex: opsmgr.example.com
opsman_domain_or_ip_address: opsman-pks-nsx.lab.alekssaul.com

# IP to assign to Ops Manager VM
om_ip: 192.168.50.2

# Netmask for Ops Manager network
om_netmask: 255.255.255.0

# Gateway for Ops Manager network
om_gateway: 192.168.50.1

# Comma separated list of DNS servers used by Ops Manager
om_dns_servers: 8.8.8.8

# SSH password for Ops Manager (ssh user is ubuntu)
opsman_ssh_password: "Pivotal"

# Comma-separated list of NTP Servers
om_ntp_servers: time.google.com

# Disk type for Ops Manager VM (thick|thin)
opsman_disk_type: "thin"

# Whether to power on Ops Manager VM after creation
om_vm_power_state: true

# PCF Ops Manager minor version to track
opsman_major_minor_version: ^2\.4\.[0-9]+$ # Ops Manager minor version to track (e.g ^2\.0\.[0-9]+$ will track 2.0.x versions)

# Pivnet token for downloading resources from Pivnet. Find this token at https://network.pivotal.io/users/dashboard/edit-profile
pivnet_token: "teafdsa"

# opsman_admin_username/opsman_admin_password needs to be specified
opsman_admin_username: admin       # Username for Ops Manager admin account
opsman_admin_password: "Pivotal"       # Password for Ops Manager admin account

# AZ configuration for Ops Director
az_1_cluster_name: cluster01            # Name of cluster in vCenter for AZ1
az_1_name: az1                    # Logical name of availability zone. No spaces or special characters.
az_1_rp_name: pks-az1                 # Resource pool name in vCenter for AZ1
az_2_cluster_name: cluster01            # Name of cluster in vCenter for AZ2
az_2_name: az2                    # Logical name of availability zone. No spaces or special characters.
az_2_rp_name: pks-az2                 # Resource pool name in vCenter for AZ2
az_3_cluster_name: cluster01            # Name of cluster in vCenter for AZ3
az_3_name: az3                    # Logical name of availability zone. No spaces or special characters.
az_3_rp_name: pks-az3                 # Resource pool name in vCenter for AZ3

vm_disk_type: "thin"

ephemeral_storage_names: datastore2
persistent_storage_names: datastore2

# vSphere datastore folder (such as pcf_disk) where attached disk images will be created
bosh_disk_path: "pks_disk"

# vSphere datacenter folder (such as pcf_templates) where templates will be placed
bosh_template_folder: "pks_templates"

# vSphere datacenter folder (such as pcf_vms) where VMs will be placed
bosh_vm_folder: "pks_vms"

icmp_checks_enabled: false

# Infrastructure Configuration
infra_excluded_range: 192.168.50.0-192.168.50.10         # Infrastructure network exclusion range
infra_network_name: "pks-infrastructure"
infra_nw_azs: az1,az2,az3                  # Comma separated list of AZ’s to be associated with this network
infra_nw_cidr: 192.168.50.0/24               # Infrastructure network CIDR, ex: 10.0.0.0/22
infra_nw_dns: 8.8.8.8                 # Infrastructure network DNS
infra_nw_gateway: 192.168.50.1             # Infrastructure network Gateway
infra_vsphere_network: PKS-Infra       # vCenter Infrastructure network name

services_excluded_range: 192.168.60.0-192.168.60.10        # Services network exclusion range
services_network_name: "pks-services"
services_nw_azs: az1,az2,az3              # Comma separated list of AZ’s to be associated with this network
services_nw_cidr: 192.168.60.0/24             # Services network CIDR, ex: 10.0.0.0/22
services_nw_dns: 8.8.8.8              # Services network DNS
services_nw_gateway: 192.168.60.1          # Services network Gateway
services_vsphere_network: PKS-Services     # vCenter Services network name

enable_vm_resurrector: true
max_threads: 30
trusted_certificates:
nsx_networking_enabled: false
pks_api_domain: pksapi.lab.alekssaul.com

pks_major_minor_version: ^1\.3\.[0-9]+$ # Ops Manager minor version to track (e.g ^2\.0\.[0-9]+$ will track 2.0.x versions)

pks_cli_username: pksuaa
pks_cli_password: "Pivotal123!"
pks_cli_useremail: asaul@pivotal.io

pks_cluster_name: pksvsphere01
pks_cluster_externalhostname: pksvsphere01.lab.alekssaul.com

######## AZs and Networks configuration for the tile
pks-networks: |
  network:
    name: ((infra_network_name))
  other_availability_zones:
  - name: ((az_1_name))
  - name: ((az_2_name))
  - name: ((az_3_name))
  service_network:
    name: ((services_network_name))
  singleton_availability_zone:
    name: ((az_1_name))

pks-properties: |
  ######## Certificate to secure the PKS API
  .pivotal-container-service.pks_tls:
    #
    # The "generate_cert_domains" parameter controls the automatic certificates
    #   generation behavior for this property.
    #
    # If auto-generation of certs is desired, leave this parameter un-commented
    #   and update the array of domain names to be used for cert generation.
    #   e.g. ["*.pks.mydomain.com","*.api.pks.mydomain.com"].
    #   Leave parameters cert_pem and private_key_pem with empty values in this case.
    #
    # Otherwise, either comment out or delete this parameter line and
    #   provide the certificate (cert_pem) and private key (private_key_pem) values
    #   in the corresponding parameters further below.
    #
    generate_cert_domains: ["((pks_api_domain))"]
    value:
      cert_pem: | 
        -----BEGIN CERTIFICATE-----
        MIIEUDCCArigAwIBAgIQd4ou4/eV8Sk1iMlf/bSNsjANBgkqhkiG9w0BAQsFADB5
        MR4wHAYDVQQKExVta2NlcnQgZGV2ZWxvcG1lbnQgQ0ExJzAlBgNVBAsMHmFzYXVs
        QEFsZWtzcy1NYWNCb29rLVByby5sb2NhbDEuMCwGA1UEAwwlbWtjZXJ0IGFzYXVs
        QEFsZWtzcy1NYWNCb29rLVByby5sb2NhbDAeFw0xODEwMjkxNzEyMzZaFw0yODEw
        MjkxNzEyMzZaMFIxJzAlBgNVBAoTHm1rY2VydCBkZXZlbG9wbWVudCBjZXJ0aWZp
        ggGBAA7FwTdBJQy3MjORsA23w6R5qSf3BEWEV+djdu3NytYYbf87+ua75/abv8tM
        35rco7EQhuUrX4lcfTPwj3iTp+PsU540yoCEgw1k68cCpQY/ihaY3Rq1T13gNhNy
        lR//K9Uqo/jK2OjJGQqJpLmnwCfva8JRh5Z1pjT2LcDI/SPkJXp39u0uFCmrJ2gA
        og7EJZ1jsE8xm4jm0lbPB7vzrdwe2WpBbzYkiyPFJsjIUAN1dM/eEjDC2QUFhF3L
        RevaZGiJD0Ye8abFlVI3ogqyQzK6zdLkx91Y5SFE1wl7oCYXqvgTYJYqX05X0XxN
        RSe5S13prLJVlaT1vxUrqjPBm+mUIKKC9nMnmc6e4QDXPLiVKBaM1dcojOD57y31
        GnhZNsBKk2JM2gvdIZ1Vm4xut/vq1lz6jxYGZQyEwOaqRxJz1LY3DEB8Tq8uk/uK
        22I4W3IvT8BIfoDFEUpZMbWf6BaVwJGwYJf7/v8w0jBIwvA0Yt9qs/aWRaa5qDpT
        J5vVqw==
        -----END CERTIFICATE-----
      private_key_pem: |
        -----BEGIN PRIVATE KEY-----
        MIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQC46pe2O+hmHC19
        IiVA4XBQdR030S1Rw9kOx1xZBjS7Ht2cJp6W9RbERmZwuP11p9PqYOPEvMMXccul
        QERHOSv+UQ3RW/KKrlD6dN4Im1ltSv0YknyslU1Tfg4joHmnxA57QmGKLSBkOOX1
        BYsPNEC4/nWhtH+HVX42I0fo08iaL7JTqMTCROzBE0yrbtgNEvgnC2CXq+FbEbO/
        gIRlGCoaPMx7ZzBo5Tk87WcHH4bt3psuTClt4z5esPAuZORF/g7UjtwWuYlGUCOk
        i5AY6NvjTlNuaUaB8sjyHOYNRHIn32ksPnDk23eiEpEwc2SG+nQJBkK3xqbbYJfR
        IlYgq8kY/QKBgBQCQOODmiftjTZHlXdsVdD1eGcmOYrjqnd0LFPtQ11EN55oGNbG
        auqjE6FT6BPxDLnEMJ6TptFpgsfmpMwwhJHrMwJzYqEkhc4vjBd//loJPVyzKQcd
        ApXA2Faj9H7fLKOxxH+BNTD0LPKH7zSydv28YIAdmZgmP6t3qfiGbxepAoGAfZ4k
        7IllqNQZu8RFhJImvnwiHblkv010IYl7yJLOK0Uil4utQivjTOfGUXRZLDO2yXYF
        fxkx3VECMlyXNnl2WIYBzmKTAJ64oCvmmb64O1+luePPCRBmbtF/kBxmY5xn3ZMB
        ZSrPpVLQHdykVoLNObMmk9stbDYchIjtbyfcnoUCgYBniEceorrAJfQPTBut9BqR
        8WJfELn6jQyaM+T7zrecaXPKkFiNh0zZSJiJulUtpo4x5GP7dDARZm+VNACZ2+8c
        mxmPNBFTHFombDONQv3KEXVsSAmHEwXJWETM/mnlkLmTLsc1xZPTDOEj2AcQnTHI
        cSZ+NXrHWXfNXmbk0zFDmw==
        -----END PRIVATE KEY-----
  ## API Hostname (FQDN)
  .properties.pks_api_hostname:
    value: ((pks_api_domain))
  .properties.worker_max_in_flight:
    value: 2
  ######## Configuration for Plan 1
  .properties.plan1_selector:
    value: "Plan Active"
  .properties.plan1_selector.active.name:
    value: "small"  # the name that appears for end users to choose
  .properties.plan1_selector.active.description:
    value: "Small plan"
  .properties.plan1_selector.active.master_az_placement:
    value: [((az_1_name))]  # Specify the availability zones you want your master and ETCD nodes spread across equally. If only one master/ETCD has been selected, choose a single AZ for your singleton job.
  .properties.plan1_selector.active.worker_az_placement:
    value: [((az_1_name)),((az_2_name)),((az_3_name))]  # Specify the availability zones you want your worker nodes spread across equally.
  .properties.plan1_selector.active.master_instances:
    value: 1
  .properties.plan1_selector.active.master_vm_type:
    value: medium
  .properties.plan1_selector.active.master_persistent_disk_type:
    value: "10240"
  .properties.plan1_selector.active.worker_vm_type:
    value: medium
  .properties.plan1_selector.active.worker_persistent_disk_type:
    value: "10240"
  .properties.plan1_selector.active.worker_instances:
    value: 2    # The number of K8s worker instances
  .properties.plan1_selector.active.errand_vm_type:
    value: micro
  .properties.plan1_selector.active.addons_spec:
    value: "" # Kubernetes yml that contains specifications of addons to run on every cluster. This is an experimental feature. Please consider carefully before applying this to your plan
  .properties.plan1_selector.active.allow_privileged_containers:
    value: true  # Privileged containers run with host-like permissions. Allowing your users to deploy privileged containers in clusters using this plan can create security vulnerabilities and may impact other tiles. Use with caution.
  .properties.plan1_selector.active.disable_deny_escalating_exec:
    value: true  # This admission controller will deny exec and attach commands to pods that run with escalated privileges that allow host access. If checked, the admission controller will be disabled. Clusters in this plan can then create security vulnerabilities and may impact other tiles. Use with caution.
  ######## Configuration for Plan 2 - "Plan Inactive" or "Plan Active"
  .properties.plan2_selector:
    value: "Plan Active"
  .properties.plan2_selector.active.name:
    value: "small-ha"  # the name that appears for end users to choose
  .properties.plan2_selector.active.description:
    value: "HA Plan"
  .properties.plan2_selector.active.master_az_placement:
    value: [((az_1_name)),((az_2_name)),((az_3_name))]   # Specify the AZ in which the cluster will be created
  .properties.plan2_selector.active.worker_az_placement:
    value: [((az_1_name)),((az_2_name)),((az_3_name))]   # Specify the availability zones you want your worker nodes spread across equally.
  .properties.plan2_selector.active.master_instances:
    value: 3
  .properties.plan2_selector.active.master_vm_type:
    value: medium
  .properties.plan2_selector.active.master_persistent_disk_type:
    value: "10240"
  .properties.plan2_selector.active.worker_vm_type:
    value: medium
  .properties.plan2_selector.active.worker_persistent_disk_type:
    value: "10240"
  .properties.plan2_selector.active.worker_instances:
    value: 2    # The number of K8s worker instances
  .properties.plan2_selector.active.errand_vm_type:
    value: micro
  .properties.plan2_selector.active.addons_spec:
    value: "" # Kubernetes yml that contains specifications of addons to run on every cluster. This is an experimental feature. Please consider carefully before applying this to your plan
  .properties.plan2_selector.active.allow_privileged_containers:
    value: true  # Privileged containers run with host-like permissions. Allowing your users to deploy privileged containers in clusters using this plan can create security vulnerabilities and may impact other tiles. Use with caution.
  .properties.plan2_selector.active.disable_deny_escalating_exec:
    value: true  # This admission controller will deny exec and attach commands to pods that run with escalated privileges that allow host access. If checked, the admission controller will be disabled. Clusters in this plan can then create security vulnerabilities and may impact other tiles. Use with caution.
  ######## Configuration for Plan 3 - "Plan Inactive" or "Plan Active"
  .properties.plan3_selector:
    value: "Plan Inactive"
  ### if plan 3 is to be activated, then uncomment and fill out the plan3 parameters below
  ### otherwise, leave this section commented out to avoid Ops Manager parameter settings errors
  # .properties.plan3_selector.active.name:
  #   value: "large"  # the name that appears for end users to choose
  # .properties.plan3_selector.active.description:
  #   value: "Plan 3 description"
  # .properties.plan3_selector.active.master_az_placement:
  #   value: [( ( az_1_name ) )]  # Specify the AZ in which the cluster will be created
  # .properties.plan3_selector.active.worker_az_placement:
  #   value: [( ( az_1_name ) ),( ( az_2_name ) ),( ( az_3_name ) )]  # Specify the availability zones you want your worker nodes spread across equally.
  # .properties.plan3_selector.active.master_instances:
  #   value: 3
  # .properties.plan3_selector.active.master_vm_type:
  #   value: large
  # .properties.plan3_selector.active.master_persistent_disk_type:
  #   value: "10240"
  # .properties.plan3_selector.active.worker_vm_type:
  #   value: large
  # .properties.plan3_selector.active.worker_persistent_disk_type:
  #   value: "10240"
  # .properties.plan3_selector.active.worker_instances:
  #   value: 8    # The number of K8s worker instances
  # .properties.plan3_selector.active.errand_vm_type:
  #   value: micro
  # .properties.plan3_selector.active.addons_spec:
  #   value: "" # Kubernetes yml that contains specifications of addons to run on every cluster. This is an experimental feature. Please consider carefully before applying this to your plan
  # .properties.plan3_selector.active.allow_privileged_containers:
  #   value: false  # Privileged containers run with host-like permissions. Allowing your users to deploy privileged containers in clusters using this plan can create security vulnerabilities and may impact other tiles. Use with caution.
  # .properties.plan3_selector.active.disable_deny_escalating_exec:
  #   value: false  # This admission controller will deny exec and attach commands to pods that run with escalated privileges that allow host access. If checked, the admission controller will be disabled. Clusters in this plan can then create security vulnerabilities and may impact other tiles. Use with caution.
  ######## Kubernetes cloud provider: GCP or vSphere, or AWS
  .properties.cloud_provider:
    value: vSphere
  .properties.cloud_provider.vsphere.vcenter_master_creds:
    value:
       identity: ((vcenter_usr)) # vcenter_admin_username
       password: ((vcenter_pwd)) # vcenter_admin_password
  .properties.cloud_provider.vsphere.vcenter_ip:
    value: ((vcenter_host))
  .properties.cloud_provider.vsphere.vcenter_dc:
    value: ((vcenter_datacenter))
  .properties.cloud_provider.vsphere.vcenter_ds:
    value: ((vcenter_datastore))
  .properties.cloud_provider.vsphere.vcenter_vms:
    value: ((bosh_vm_folder)) # The name should be the same as the VM Folder in the Ops Manager Director tile, under the vCenter config page.
  ### if AWS is selected, then uncomment and fill out the AWS parameters below.
  ### otherwise, leave this section commented out to avoid Ops Manager parameter settings errors
  # .properties.cloud_provider.aws.aws_access_key_id_master:
  #   value:
  # .properties.cloud_provider.aws.aws_secret_access_key_master:
  #   value:
  #     secret: "***"
  # .properties.cloud_provider.aws.aws_access_key_id_worker:
  #   value:
  # .properties.cloud_provider.aws.aws_secret_access_key_worker:
  #   value:
  #     secret: "***"
  ######## Network selector - flannel or nsx
  .properties.network_selector:
    value: nsx
  ### if NSX-T is to be used, then uncomment and fill out the nxt parameters below
  ### otherwise, leave this section commented out to avoid Ops Manager parameter settings errors
  .properties.network_selector.nsx.nsx-t-host:
    value: ((nsxt_hostname_or_ipaddress))  # Please enter the NSX Manager hostname or IP address
  #.properties.network_selector.nsx.credentials:
  #  value:
  #    identity: ( ( nsxt_admin_username ) )  # username to connect to the NSX Manager
  #    password: ( ( nsxt_admin_password ) )  # password to connect to the NSX Manager
  .properties.network_selector.nsx.nsx-t-ca-cert:
    value: "-----BEGIN CERTIFICATE-----\r\nMIIDXjCCAkagAwIBAgIGAWac8TUzMA0GCSqGSIb3DQEBCwUAMHAxITAfBgNVBAMM\r\nGG5zeG1nci5sYWIuYWxla3NzYXVsLmNvbTEQMA4GA1UECgwHQ29tcGFueTESMBAG\r\nA1UECwwJbmV0LWludGVnMQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExCzAJBgNV\r\nBAcMAlNGMB4XDTE4MTAyMjE4MDIyNVoXDTI4MTAxOTE4MDIyNVowcDEhMB8GA1UE\r\nAwwYbnN4bWdyLmxhYi5hbGVrc3NhdWwuY29tMRAwDgYDVQQKDAdDb21wYW55MRIw\r\nEAYDVQQLDAluZXQtaW50ZWcxCzAJBgNVBAYTAlVTMQswCQYDVQQIDAJDQTELMAkG\r\nA1UEBwwCU0YwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQChdmle0ZOZ\r\nREkuaX/AsEFMdB1x99LvCDgPe1M8FXPo1bXADAb0fnWhPhmLop311j6bIuYa6tXb\r\nuwCGDjELuhx2D6yCm0FoGVYXE78M1VDuNDOO5jDX8tclT8YqNgmjWrftLP8J8UWn\r\nrLby+pBsS/Cx9cz1zXOEC2+5HoxkDKsxJe4gOp8TtjwU8kyZ5ed4WQ18Yfyoo0Ro\r\nV8mfMwP1RTuSjnLE4sbpu20S4W3yvdlXmC8EMUSPBwSarWxDqEUfFFzey9qP8i++\r\nEXQwtIPO/DA9Fyr0qgO67Ulh0UjI0MGOur4ZjSBDU80eGkadPb0/ZttYBhRRtPmh\r\nP2lhDkx3OmTLAgMBAAEwDQYJKoZIhvcNAQELBQADggEBAHJmZf9/CgBfr3OBYRgN\r\nIUmYIo+nNAeBTXSfuZPD1Ut35fvuiZuEcxuB+Ip0IhY7O2j6lkWZeNkl8LH7Vv15\r\nkGvhLF+Lkkvsw7K6EFz8kRU5Y5OkUSb7sBew/3Uox6YLzsqnoUk+xLipI2TMimzJ\r\n/3QD/GocBp5S9jv/iy+8wPwEkv08gcS/VKHZMRZmQTnY122nbhzDl6FcI+E+dnx+\r\nbgApuxi09Iapu1TQl4b7zC0uZIdwsMZX87jGEDS2sZca+gifd/Rgg9Mt0V2KRrpk\r\n5idEaHgxzE+j7rlkevtP+3UDoKWRt8bW/L3YSaAN2bQJymmCQhpRlGdod19GQmc0\r\nlzE=\r\n-----END
      CERTIFICATE-----"  # Optional custom CA certificate to be used to connect to NSX Manager
  #     -----BEGIN CERTIFICATE-----
  #     ...
  #     -----END CERTIFICATE-----
  .properties.network_selector.nsx.vcenter_cluster:
    value: ((nsxt_vcenter_cluster))   # Please enter the vSphere cluster name where PKS will be deployed
  .properties.network_selector.nsx.nsx-t-insecure:
    value: true    # whether to disable SSL certificate verification when connecting to the NSX Manager
  .properties.network_selector.nsx.t0-router-id:
    value: 45d1a1fc-719a-4c10-9c6c-67b0da5944b4  # the UUID of the Tier-0 logical router configured in NSX manager
  .properties.network_selector.nsx.ip-block-id:
    value: ebd0df50-9c60-4a34-a611-7ce7898e2211  # the UUID of the IP Block configured in NSX manager
  .properties.network_selector.nsx.floating-ip-pool-ids:
    value: 811af49e-d82a-4713-b8e7-78defb381126  # the UUID of the Floating IP Pool configured in NSX manager (for the current release only a single ID is supported)
  .properties.network_selector.nsx.network_automation:
    value: true     # Automated Network Provisioning
  .properties.network_selector.nsx.nat_mode:
    value: true
  .properties.network_selector.nsx.nodes-ip-block-id:
    value: 25ca25e6-c99b-4e82-b668-eefac6e28858              # Nodes IP Block ID
  # .properties.network_selector.nsx.bosh-client-id:
  #   value:
  # .properties.network_selector.nsx.bosh-client-secret:
  #   value:
  #     secret: "***"
  # .properties.proxy_selector:
  #   value: Disabled      # HTTP/HTTPS Proxy (for vSphere only) - Enabled or Disabled
  # .properties.proxy_selector.enabled.http_proxy_url:
  #   value:
  # .properties.proxy_selector.enabled.http_proxy_credentials:
  #   value:
  #     password: "***"
  # .properties.proxy_selector.enabled.no_proxy:
  #   value:
  .properties.vm_extensions:
    value:           # Allow outbound internet access from Kubernetes cluster vms (IaaS-dependent)
    - public_ip      # Enable outbound internet access. Warning: Not allowing internet access will require a NAT instance.
  ######## UAA configuration page
  .properties.uaa_pks_cli_access_token_lifetime:
    value: 86400
  .properties.uaa_pks_cli_refresh_token_lifetime:
    value: 172800
  .properties.uaa_oidc:
    value: true
  ## Configure your UAA user account store with either internal or external authentication mechanisms
  ## Accepted values:
  ## - internal    (Internal UAA)
  ## - ldap        (LDAP Server)
  .properties.uaa:
    value: ldap
  ### if ldap server, then uncomment and fill out the ldap parameters below
  ### otherwise, leave this section commented out to avoid Ops Manager parameter settings errors
  .properties.uaa.ldap.url:
    value: "ldaps://ldap.jumpcloud.com:636"
  .properties.uaa.ldap.credentials:
    value:
      identity: "uid=pksbind,ou=Usersdc=jumpcloud,dc=com"
      password: ""
  .properties.uaa.ldap.search_base:
    value: "ou=Users,dc=jumpcloud,dc=com"
  .properties.uaa.ldap.search_filter:
    value: uid={0}
  .properties.uaa.ldap.group_search_base:
    value: "ou=Users,dc=jumpcloud,dc=com"
  .properties.uaa.ldap.group_search_filter:
    value: member={0}
  # .properties.uaa.ldap.server_ssl_cert:
  #   value:
  # .properties.uaa.ldap.server_ssl_cert_alias:
  #   value:
  .properties.uaa.ldap.mail_attribute_name:
    value: mail
  # .properties.uaa.ldap.email_domains:
  #   value:
  .properties.uaa.ldap.first_name_attribute:
    value: "givenName"
  .properties.uaa.ldap.last_name_attribute:
    value: "sn"
  # .properties.uaa.ldap.ldap_referrals:
  #   value: follow
  ######## MONITORING
  ## Wavefront Integration - enabled or disabled
  .properties.wavefront:
    value: enabled
  ### if wavefront integration is enabled, uncomment and fill out the wavefront parameters below
  ### otherwise, leave this section commented out to avoid Ops Manager parameter settings errors
  .properties.wavefront.enabled.wavefront_api_url:
    value: "https://api.wavefront.com"
  .properties.wavefront.enabled.wavefront_token:
    value:
      secret: "2cbebb51"
  .properties.wavefront.enabled.wavefront_alert_targets:
    value: "asaul@pivotal.io"
  ######## LOGGING
  ######## SYSLOG - disabled or enabled
  .properties.syslog_migration_selector:
    value: disabled
  ### if syslog is enabled, uncomment and fill out the syslog parameters below
  ### otherwise, leave this section commented out to avoid Ops Manager parameter settings errors
  # .properties.syslog_migration_selector.enabled.address:
  #   value: 10.10.10.10  #The address or host for the syslog server
  # .properties.syslog_migration_selector.enabled.port:
  #   value: 0  # The port on which the syslog server listens
  # .properties.syslog_migration_selector.enabled.transport_protocol:
  #   value: tcp  # The transport protocol used to send syslog messages to the server
  # .properties.syslog_migration_selector.enabled.tls_enabled:
  #   value: true # Send logs encrypted to syslog server via TLS
  # .properties.syslog_migration_selector.enabled.permitted_peer:
  #   value: "*.example.com" # Either the accepted fingerprint (SHA1) or name of remote peer
  # .properties.syslog_migration_selector.enabled.ca_cert:
  #   value: |
  #     -----BEGIN CERTIFICATE-----
  #     ...
  #     -----END CERTIFICATE-----
  ###
  ### VMware vRealize Log Insight Integration - disabled or enabled
  .properties.pks-vrli:
    value: disabled
  ### if vRealize Log Insight Integration is enabled, uncomment and fill out the corresponding parameters below
  ### otherwise, leave this section commented out to avoid Ops Manager parameter settings errors
  # .properties.pks-vrli.enabled.host:
  #   value:
  # .properties.pks-vrli.enabled.use_ssl:
  #   value: true
  # .properties.pks-vrli.enabled.skip_cert_verify:
  #   value: false
  # .properties.pks-vrli.enabled.ca_cert:
  #   value:
  # .properties.pks-vrli.enabled.rate_limit_msec:
  #   value: 0
  #
  ######## TELEMETRY
  ## Information about PKS configuration and usage can be collected to provide better support and product improvements: enabled or disabled
  .properties.telemetry_selector:
    value: enabled

######## Resources
pks-resources: |
  pivotal-container-service:
    instance_type:
      id: micro
    persistent_disk:
      size_mb: "10240"

